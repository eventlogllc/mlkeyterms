    "what": [
      "The tendency for the gradients of early hidden layers of some deep neural networks to become surprisingly flat (low). Increasingly lower gradients result in increasingly smaller changes to the weights on nodes in a deep neural network, leading to little or no learning. Models suffering from the vanishing gradient problem become difficult or impossible to train. Long Short-Term Memory cells address this issue.",
      "Compare to exploding gradient problem."
      "[英] [related] tendency, gradients, early, hidden layer, become, flat, lower gradients, result, smaller weights, nodes, deep, neural network, no learning, Models, suffering, vanishing gradient problem, difficult, impossible to train, long short-term, Memory cells, address this issue",
      "[和] [関連] 傾向, 勾配 , 早い, 隠しレイヤー, 成る, 平, より低い勾配, 結果, 小さいウェイト, 節点, 深い, 神経網, に導く, 学習なし, モデル, 苦しむ, 勾配消失問題, 難しい, 訓練することは不可能, 長期短期, メモリーセル, この問題に対処する",
      "[漢] 趨勢，梯度，早期，隱藏層，變得，平坦，梯度較低，結果，較小的權重，節點，深度，神經網絡，無學習，模型，痛苦，梯度消失問題，困難，不可能訓練，長期短期， 記憶細胞，解決這個問題",
    ],
