    "what": [
      "A function (for example, ReLU or sigmoid) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.",
      "Activation functions add nonlinearity to neural networks. The more nonlinear our decision function, the more complex decisions it can make.",
       "[英] [related] [A function], [(for example, ReLU or sigmoid)], [takes in the weighted sum of all of the inputs], [from the previous layer], [and then generates and passes an output value], [(typically nonlinear)], [to the next layer.]",
      "[和] [関数], [（たとえば、ReLUまたはシグモイド）], [すべての入力の加重和を取ります], [前のレイヤーから], [そして出力値を生成して渡します], [（通常は非線形）], [次のレイヤーへ]",	    
      "[漢] [相關], [一個函數], [（例如，ReLU 或 sigmoid）], [獲取所有輸入的加權和], [來自上一層], [然後生成並傳遞一個輸出值], [（通常是非線性的）], [到下一層]",
    ],
